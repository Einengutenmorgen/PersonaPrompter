{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os \n",
    "\n",
    "import re\n",
    "\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persona Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hilfsfunktionen (unverändert) ---\n",
    "\n",
    "\n",
    "\n",
    "def load_history_tweets_for_user(user_id_str: str, jsonl_dir: Path) -> list:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Lädt die History-Tweets für einen gegebenen Nutzer aus seiner JSONL-Datei.\n",
    "\n",
    "    Gibt eine Liste von Tweet-Texten zurück.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    user_file = jsonl_dir / f\"{user_id_str}.jsonl\"\n",
    "\n",
    "    history_tweet_texts = []\n",
    "\n",
    "\n",
    "\n",
    "    if not user_file.exists():\n",
    "\n",
    "        print(f\"FEHLER: Datendatei für Nutzer {user_id_str} nicht gefunden unter {user_file}\")\n",
    "\n",
    "        return history_tweet_texts\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(user_file, 'r', encoding='utf-8') as f:\n",
    "\n",
    "            for line_number, line in enumerate(f):\n",
    "\n",
    "                if line.strip() == \"\":\n",
    "\n",
    "                    continue \n",
    "\n",
    "                try:\n",
    "\n",
    "                    data = json.loads(line)\n",
    "\n",
    "                    if data.get(\"set\") == \"history\":\n",
    "\n",
    "                        tweets = data.get(\"tweets\", [])\n",
    "\n",
    "                        for tweet in tweets:\n",
    "\n",
    "                            if 'full_text' in tweet and tweet['full_text']:\n",
    "\n",
    "                                history_tweet_texts.append(tweet['full_text'])\n",
    "\n",
    "                        break \n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "\n",
    "                    print(f\"WARNUNG: JSON-Dekodierungsfehler in Zeile {line_number + 1} der Datei {user_file}: {e}\")\n",
    "\n",
    "                    continue\n",
    "\n",
    "        \n",
    "\n",
    "        if not history_tweet_texts:\n",
    "\n",
    "            print(f\"WARNUNG: Keine History-Tweets für Nutzer {user_id_str} im 'history'-Set gefunden oder 'full_text' fehlt.\")\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"FEHLER beim Lesen der Datei {user_file}: {e}\")\n",
    "\n",
    "    \n",
    "\n",
    "    return history_tweet_texts\n",
    "\n",
    "\n",
    "\n",
    "def format_tweets_for_llm(tweet_texts: list, max_tweets: int = 50, max_chars: int = 10000) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Formatiert eine Liste von Tweet-Texten zu einem einzigen String für den LLM-Input.\n",
    "\n",
    "    Begrenzt die Anzahl der Tweets und die Gesamtzeichenzahl, um Kontextfenster nicht zu sprengen.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    selected_tweets = tweet_texts[:max_tweets]\n",
    "\n",
    "    \n",
    "\n",
    "    formatted_string = \"\"\n",
    "\n",
    "    for tweet in selected_tweets:\n",
    "\n",
    "        if len(formatted_string) + len(tweet) + len(\"\\n---\\n\") > max_chars:\n",
    "\n",
    "            break\n",
    "\n",
    "        formatted_string += tweet + \"\\n---\\n\"\n",
    "\n",
    "        \n",
    "\n",
    "    if not formatted_string and tweet_texts: \n",
    "\n",
    "        print(f\"WARNUNG: Konnte Tweets nicht formatieren. Möglicherweise sind einzelne Tweets zu lang oder max_chars zu klein.\")\n",
    "\n",
    "        if tweet_texts[0] and len(tweet_texts[0]) <= max_chars:\n",
    "\n",
    "            return tweet_texts[0]\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "    return formatted_string.strip().rstrip(\"-\")\n",
    "\n",
    "\n",
    "\n",
    "def get_meta_prompt_for_persona_generation_json(num_candidates: int) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Gibt den Meta-Prompt für das LLM zur Generierung von Persona-Kandidaten\n",
    "\n",
    "    im JSON-Format zurück.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    meta_prompt = f\"\"\"\n",
    "\n",
    "Sie sind ein KI-Assistent, der detaillierte Persona-Beschreibungen erstellt.\n",
    "\n",
    "Basierend auf den folgenden Tweets eines Nutzers, analysieren Sie dessen Schreibstil, typische Themen, Tonalität und Interaktionsmuster.\n",
    "\n",
    "\n",
    "\n",
    "Generieren Sie {num_candidates} unterschiedliche Persona-Beschreibungen.\n",
    "\n",
    "Jede Beschreibung sollte ein prägnanter Absatz sein, der, wenn er einem Sprachmodell gegeben wird, dieses befähigen würde, den Twitter-Stil des Nutzers zu imitieren.\n",
    "\n",
    "\n",
    "\n",
    "Konzentrieren Sie sich bei Ihrer Analyse und in den Persona-Beschreibungen auf diese Aspekte:\n",
    "\n",
    "1.  **Schreibstil:** Satzstruktur, Vokabular (formell/informell, Umgangssprache, Fachbegriffe), Verwendung von Interpunktion, Groß-/Kleinschreibung, Emojis, Hashtags, Links.\n",
    "\n",
    "2.  **Häufige Themen:** Über welche Themen twittert der Nutzer häufig? Was sind wiederkehrende Interessen oder Meinungen?\n",
    "\n",
    "3.  **Tonalität:** Ist der Nutzer im Allgemeinen positiv, negativ, neutral, sarkastisch, humorvoll, analytisch, kritisch, unterstützend etc.?\n",
    "\n",
    "4.  **Interaktionsmuster:** Wie interagiert der Nutzer in Antworten (falls Beispiele vorhanden sind)? Ist er gesprächig, argumentativ, unterstützend? Initiiert er oft Gespräche oder antwortet er hauptsächlich?\n",
    "\n",
    "\n",
    "\n",
    "ANWEISUNGEN FÜR DAS AUSGABEFORMAT:\n",
    "\n",
    "Ihre Antwort MUSS ein valides JSON-Objekt sein und sonst nichts.\n",
    "\n",
    "Das JSON-Objekt soll einen einzigen Schlüssel namens \"persona_descriptions\" enthalten.\n",
    "\n",
    "Der Wert dieses Schlüssels soll eine JSON-Liste von Strings sein. Jeder String in dieser Liste ist eine der {num_candidates} generierten Persona-Beschreibungen.\n",
    "\n",
    "Fügen Sie keine einleitenden Sätze, Erklärungen oder Text außerhalb des JSON-Objekts hinzu.\n",
    "\n",
    "\n",
    "\n",
    "Beispiel für das erwartete JSON-Format (bei num_candidates = 2):\n",
    "\n",
    "{{\n",
    "\n",
    "  \"persona_descriptions\": [\n",
    "\n",
    "    \"Dies ist die erste Persona-Beschreibung, die den Schreibstil, die Themen und die Tonalität des Nutzers X zusammenfasst...\",\n",
    "\n",
    "    \"Die zweite Persona-Beschreibung für Nutzer X könnte sich auf andere Aspekte konzentrieren oder eine alternative Interpretation bieten...\"\n",
    "\n",
    "  ]\n",
    "\n",
    "}}\n",
    "\n",
    "\n",
    "\n",
    "Hier sind die Tweets des Nutzers:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    return meta_prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def call_gemini_api_for_persona_gen(full_prompt_for_llm: str, num_candidates: int) -> list:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Ruft die Google Gemini API auf, um Persona-Prompt-Kandidaten als JSON zu generieren.\n",
    "\n",
    "    Gibt eine Liste von Strings zurück, wobei jeder String \"Persona X: Beschreibung...\" ist.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- SENDE PROMPT AN GOOGLE GEMINI API (JSON Modus) ---\")\n",
    "\n",
    "    # print(full_prompt_for_llm) # Optional für Debugging\n",
    "\n",
    "    print(\"--- WARTE AUF ANTWORT VON GEMINI API ... ---\")\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Stellen Sie sicher, dass Ihr API-Schlüssel konfiguriert ist, z.B.:\n",
    "\n",
    "        # api_key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "        # if not api_key:\n",
    "\n",
    "        #     raise ValueError(\"API Key nicht in Umgebungsvariablen gefunden.\")\n",
    "\n",
    "        # genai.configure(api_key=api_key) # Idealerweise einmal zu Beginn Ihres Skripts/Notebooks\n",
    "\n",
    "\n",
    "\n",
    "        model = genai.GenerativeModel(model_name='gemini-1.5-flash-latest') # oder Ihr bevorzugtes Modell\n",
    "\n",
    "\n",
    "\n",
    "        generation_config = genai.types.GenerationConfig(\n",
    "\n",
    "            candidate_count=1,\n",
    "\n",
    "            max_output_tokens=20, # Passen Sie dies bei Bedarf an\n",
    "\n",
    "            temperature=0.7,\n",
    "\n",
    "            response_mime_type=\"application/json\"  # WICHTIG: JSON-Ausgabe anfordern\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        safety_settings = {\n",
    "\n",
    "            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "\n",
    "            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "\n",
    "            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "\n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        response = model.generate_content(\n",
    "\n",
    "            contents=full_prompt_for_llm,\n",
    "\n",
    "            generation_config=generation_config,\n",
    "\n",
    "            safety_settings=safety_settings\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        print(\"--- ANTWORT VON GEMINI API ERHALTEN (JSON Modus) ---\")\n",
    "\n",
    "\n",
    "\n",
    "        if not response.text: # type: ignore\n",
    "\n",
    "            print(\"WARNUNG: Kein Text von der Gemini API erhalten (JSON Modus).\")\n",
    "\n",
    "            if response.prompt_feedback and response.prompt_feedback.block_reason: # type: ignore\n",
    "\n",
    "                print(f\"Blockierungsgrund: {response.prompt_feedback.block_reason}\") # type: ignore\n",
    "\n",
    "            return []\n",
    "\n",
    "        \n",
    "\n",
    "        # Für Debugging kann es hilfreich sein, den Rohtext auszugeben\n",
    "\n",
    "        # print(f\"--- ROH-ANTWORT VON GEMINI (sollte JSON sein) ---\\n{response.text}\")\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            parsed_json = json.loads(response.text) # type: ignore\n",
    "\n",
    "            raw_descriptions = parsed_json.get(\"persona_descriptions\")\n",
    "\n",
    "\n",
    "\n",
    "            if not isinstance(raw_descriptions, list):\n",
    "\n",
    "                print(f\"WARNUNG: JSON-Struktur unerwartet. 'persona_descriptions' ist keine Liste oder fehlt. Erhalten: {parsed_json}\")\n",
    "\n",
    "                # Fallback: Versuchen, den Rohtext als einzelne Persona zu verwenden, falls möglich\n",
    "\n",
    "                if isinstance(parsed_json, str): # Manchmal gibt das Modell trotz allem nur einen String zurück\n",
    "\n",
    "                     return [f\"Persona 1: {parsed_json.strip()}\"]\n",
    "\n",
    "                return []\n",
    "\n",
    "\n",
    "\n",
    "            # Validieren, dass die Liste Strings enthält und leere Strings filtern\n",
    "\n",
    "            persona_candidates_from_json = [str(desc).strip() for desc in raw_descriptions if isinstance(desc, str) and str(desc).strip()]\n",
    "\n",
    "            \n",
    "\n",
    "            if not persona_candidates_from_json and raw_descriptions:\n",
    "\n",
    "                 print(f\"WARNUNG: 'persona_descriptions' enthielt Elemente, aber keine validen Strings nach Filterung. Original: {raw_descriptions}\")\n",
    "\n",
    "                 return []\n",
    "\n",
    "            \n",
    "\n",
    "            if not persona_candidates_from_json:\n",
    "\n",
    "                print(\"WARNUNG: Keine Persona-Beschreibungen im JSON gefunden.\")\n",
    "\n",
    "                return []\n",
    "\n",
    "\n",
    "\n",
    "            # Formatieren der Beschreibungen zu \"Persona X: ...\" für Konsistenz mit dem Rest des Codes\n",
    "\n",
    "            # (falls das LLM dies nicht bereits im String tut)\n",
    "\n",
    "            formatted_candidates = []\n",
    "\n",
    "            for i, desc in enumerate(persona_candidates_from_json[:num_candidates]): # Nur die angeforderte Anzahl\n",
    "\n",
    "                # Entferne ein eventuell vom LLM hinzugefügtes \"Persona X:\" Muster, um Doppelung zu vermeiden\n",
    "\n",
    "                cleaned_desc = re.sub(r\"^(Persona\\s*\\d*\\s*:)\\s*\", \"\", desc, flags=re.IGNORECASE).strip()\n",
    "\n",
    "                if cleaned_desc: # Nur hinzufügen, wenn nach der Bereinigung Text übrig ist\n",
    "\n",
    "                    formatted_candidates.append(f\"Persona {i + 1}: {cleaned_desc}\")\n",
    "\n",
    "            \n",
    "\n",
    "            if not formatted_candidates and persona_candidates_from_json:\n",
    "\n",
    "                 print(\"WARNUNG: Nach der Formatierung waren keine Kandidaten mehr übrig. Verwende Roh-JSON-Strings.\")\n",
    "\n",
    "                 # Fallback auf die ursprünglichen JSON-Strings mit \"Persona X:\" Präfix\n",
    "\n",
    "                 return [f\"Persona {i+1}: {desc_orig}\" for i, desc_orig in enumerate(persona_candidates_from_json[:num_candidates])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"--- {len(formatted_candidates)} Persona-Kandidaten erfolgreich aus JSON geparst und formatiert ---\")\n",
    "\n",
    "            return formatted_candidates\n",
    "\n",
    "\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "\n",
    "            print(f\"FEHLER: Konnte JSON-Antwort von Gemini nicht parsen: {e}\")\n",
    "\n",
    "            print(f\"Rohtext, der zum Fehler führte:\\n{response.text}\") # type: ignore\n",
    "\n",
    "            # Als Fallback könnten Sie hier die alte, zeilenbasierte Parsing-Logik auf response.text anwenden,\n",
    "\n",
    "            # aber das Ziel ist ja, diese zu vermeiden. Fürs Erste geben wir einen Fehler zurück.\n",
    "\n",
    "            return []\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"FEHLER bei der Kommunikation mit der Gemini API (JSON Modus): {e}\")\n",
    "\n",
    "        return []\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def generate_persona_candidates_for_user(user_id_str: str, jsonl_dir: Path, num_candidates: int) -> list:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Orchestriert das Laden von Tweets, die Aufbereitung und den LLM-Aufruf\n",
    "\n",
    "    zur Generierung von Persona-Prompt-Kandidaten für einen Nutzer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starte Generierung von Persona-Prompt-Kandidaten für Nutzer: {user_id_str}\")\n",
    "\n",
    "    \n",
    "\n",
    "    history_tweets = load_history_tweets_for_user(user_id_str, jsonl_dir)\n",
    "\n",
    "    if not history_tweets:\n",
    "\n",
    "        print(f\"Konnte keine History-Tweets für {user_id_str} laden. Überspringe Persona-Generierung.\")\n",
    "\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "    formatted_history_for_llm = format_tweets_for_llm(history_tweets)\n",
    "\n",
    "    if not formatted_history_for_llm:\n",
    "\n",
    "        print(f\"Konnte History-Tweets für {user_id_str} nicht für LLM formatieren. Überspringe Persona-Generierung.\")\n",
    "\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "    meta_prompt = get_meta_prompt_for_persona_generation_json(num_candidates)\n",
    "\n",
    "    full_prompt_for_llm = meta_prompt + \"\\n\\n\" + formatted_history_for_llm\n",
    "\n",
    "\n",
    "\n",
    "    persona_candidates = call_gemini_api_for_persona_gen(full_prompt_for_llm, num_candidates)\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"Generierung für Nutzer {user_id_str} abgeschlossen. {len(persona_candidates)} Kandidaten erhalten.\")\n",
    "\n",
    "    return persona_candidates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_persona_prompt_generation(\n",
    "\n",
    "    processing_mode: int,\n",
    "\n",
    "    user_data_dir: Path,\n",
    "\n",
    "    num_persona_candidates: int,\n",
    "\n",
    "    results_output_dir: Path,\n",
    "\n",
    "    specific_user_id_to_process: str | None = None, # type: ignore\n",
    "\n",
    "    eligible_users_file: Path | None = None # type: ignore\n",
    "\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Hauptfunktion zur Generierung von Persona-Prompts für Twitter-Nutzer.\n",
    "\n",
    "\n",
    "\n",
    "    Args:\n",
    "\n",
    "        processing_mode (int): 1 für spezifischen Nutzer, 2 für Nutzer aus Datei, 3 für alle im Verzeichnis.\n",
    "\n",
    "        user_data_dir (Path): Verzeichnis mit den JSONL-Dateien der Nutzer.\n",
    "\n",
    "        num_persona_candidates (int): Anzahl der zu generierenden Persona-Kandidaten pro Nutzer.\n",
    "\n",
    "        results_output_dir (Path): Verzeichnis zum Speichern der Ergebnisse.\n",
    "\n",
    "        specific_user_id_to_process (str, optional): Die ID des spezifischen Nutzers für Modus 1.\n",
    "\n",
    "        eligible_users_file (Path, optional): Pfad zur JSON-Datei mit Nutzer-IDs für Modus 2.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Stelle sicher, dass die Basis-Ausgabeverzeichnisse existieren\n",
    "\n",
    "    results_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # user_data_dir ist ein Eingabeverzeichnis, sollte bereits existieren.\n",
    "\n",
    "    # (user_data_dir.parent / \"users\").mkdir(parents=True, exist_ok=True) # Früher: (output_dir / \"users\")\n",
    "\n",
    "\n",
    "\n",
    "    users_to_process = []\n",
    "\n",
    "    all_processed_personas = {} \n",
    "\n",
    "\n",
    "\n",
    "    if processing_mode == 1:\n",
    "\n",
    "        if not specific_user_id_to_process:\n",
    "\n",
    "            print(\"FEHLER: PROCESSING_MODE 1 erfordert 'specific_user_id_to_process'.\")\n",
    "\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "        user_file_path = user_data_dir / f\"{specific_user_id_to_process}.jsonl\"\n",
    "\n",
    "        \n",
    "\n",
    "        # Optional: Dummy-Datei für Testzwecke (wenn benötigt, anpassen)\n",
    "\n",
    "        # if not user_file_path.exists():\n",
    "\n",
    "        #     print(f\"Hinweis: Dummy-Datei {user_file_path} wird für Testzwecke erstellt.\")\n",
    "\n",
    "        #     # ... (Logik zum Erstellen von Dummy-Dateien hier, falls gewünscht) ...\n",
    "\n",
    "\n",
    "\n",
    "        if user_file_path.exists():\n",
    "\n",
    "            print(f\"Versuche, spezifischen Nutzer zu verarbeiten: {specific_user_id_to_process}\")\n",
    "\n",
    "            if load_history_tweets_for_user(specific_user_id_to_process, user_data_dir):\n",
    "\n",
    "                users_to_process.append(specific_user_id_to_process)\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"Konnte keine History-Tweets für den spezifischen Nutzer {specific_user_id_to_process} laden.\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"Datendatei für spezifischen Nutzer {specific_user_id_to_process} nicht gefunden unter {user_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    elif processing_mode == 2:\n",
    "\n",
    "        if not eligible_users_file:\n",
    "\n",
    "            print(\"FEHLER: PROCESSING_MODE 2 erfordert 'eligible_users_file'.\")\n",
    "\n",
    "            return\n",
    "\n",
    "        \n",
    "\n",
    "        # Optional: Dummy eligible_users.json Datei (wenn benötigt, anpassen)\n",
    "\n",
    "        # if not eligible_users_file.exists():\n",
    "\n",
    "        #     # ... (Logik zum Erstellen von Dummy-Dateien hier) ...\n",
    "\n",
    "\n",
    "\n",
    "        if eligible_users_file.exists():\n",
    "\n",
    "            try:\n",
    "\n",
    "                with open(eligible_users_file, 'r', encoding='utf-8') as f:\n",
    "\n",
    "                    loaded_user_ids = json.load(f)\n",
    "\n",
    "                    if isinstance(loaded_user_ids, list) and all(isinstance(uid, str) for uid in loaded_user_ids):\n",
    "\n",
    "                        users_to_process = loaded_user_ids\n",
    "\n",
    "                        print(f\"{len(users_to_process)} Nutzer-IDs aus {eligible_users_file} geladen.\")\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        print(f\"FEHLER: Der Inhalt von {eligible_users_file} ist keine Liste von Strings.\")\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f\"FEHLER beim Laden oder Verarbeiten von {eligible_users_file}: {e}\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"FEHLER: Datei {eligible_users_file} nicht gefunden.\")\n",
    "\n",
    "\n",
    "\n",
    "    elif processing_mode == 3:\n",
    "\n",
    "        print(f\"Durchsuche Verzeichnis {user_data_dir} nach allen *.jsonl Dateien als Fallback...\")\n",
    "\n",
    "        found_user_files = list(user_data_dir.glob(\"*.jsonl\"))\n",
    "\n",
    "        if found_user_files:\n",
    "\n",
    "            users_to_process = [user_file.stem for user_file in found_user_files] \n",
    "\n",
    "            print(f\"{len(users_to_process)} .jsonl Dateien gefunden, die verarbeitet werden.\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"Keine .jsonl Dateien in {user_data_dir} gefunden.\")\n",
    "\n",
    "    \n",
    "\n",
    "    else:\n",
    "\n",
    "        print(f\"FEHLER: Unbekannter PROCESSING_MODE: {processing_mode}\")\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    if not users_to_process:\n",
    "\n",
    "        print(\"\\nKeine Nutzer für die Verarbeitung ausgewählt oder gefunden. Programm wird beendet.\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(f\"\\nFolgende Nutzer werden verarbeitet ({len(users_to_process)}): {users_to_process}\")\n",
    "\n",
    "\n",
    "\n",
    "        for user_id_str_loop in users_to_process:\n",
    "\n",
    "            print(f\"\\n================ BEARBEITE NUTZER: {user_id_str_loop} ================\")\n",
    "\n",
    "            \n",
    "\n",
    "            persona_prompt_candidates = generate_persona_candidates_for_user(\n",
    "\n",
    "                user_id_str_loop,\n",
    "\n",
    "                user_data_dir,\n",
    "\n",
    "                num_persona_candidates\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "            if persona_prompt_candidates:\n",
    "\n",
    "                all_processed_personas[user_id_str_loop] = persona_prompt_candidates\n",
    "\n",
    "                print(f\"\\n--- Generierte Persona-Prompt-Kandidaten für Nutzer {user_id_str_loop} ---\")\n",
    "\n",
    "                for i, candidate in enumerate(persona_prompt_candidates):\n",
    "\n",
    "                    print(f\"Kandidat {i+1}:\\n{candidate}\\n\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"Keine Persona-Prompt-Kandidaten für Nutzer {user_id_str_loop} generiert oder ein Fehler ist aufgetreten.\")\n",
    "\n",
    "        \n",
    "\n",
    "        if all_processed_personas:\n",
    "\n",
    "            output_all_personas_file = results_output_dir / \"all_generated_persona_candidates.json\"\n",
    "\n",
    "            try:\n",
    "\n",
    "                with open(output_all_personas_file, 'w', encoding='utf-8') as f:\n",
    "\n",
    "                    json.dump(all_processed_personas, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "                print(f\"\\nAlle generierten Persona-Kandidaten wurden in {output_all_personas_file} gespeichert.\")\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f\"FEHLER beim Speichern der gesammelten Persona-Kandidaten: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nProgramm-Ausführung beendet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imitaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define paths and parameters\n",
    "\n",
    "personas_input_file = DEFAULT_RESULTS_OUTPUT_DIR / \"all_generated_persona_candidates.json\"\n",
    "\n",
    "imitations_output_file_v2 = DEFAULT_RESULTS_OUTPUT_DIR / \"tweet_imitations_replies_and_completions.json\"\n",
    "\n",
    "user_data_dir_for_holdout = DEFAULT_JSONL_USER_DATA_DIR\n",
    "\n",
    "\n",
    "\n",
    "NUM_REPLY_IMITATIONS_PER_PERSONA = 3\n",
    "\n",
    "NUM_COMPLETION_IMITATIONS_PER_PERSONA = 3\n",
    "\n",
    "\n",
    "\n",
    "# Für Tweet Completion Strategie\n",
    "\n",
    "COMPLETION_FRAGMENT_NUM_WORDS = 7  # How many words from the start of the original tweet\n",
    "\n",
    "MIN_WORDS_FOR_COMPLETION_ELIGIBILITY = COMPLETION_FRAGMENT_NUM_WORDS + 3 # Tweet needs to be longer\n",
    "\n",
    "\n",
    "\n",
    "# 2. Run the imitation generation\n",
    "\n",
    "# Make sure the persona file exists from the previous script run\n",
    "\n",
    "if personas_input_file.exists():\n",
    "\n",
    "    run_imitation_generation(\n",
    "\n",
    "        personas_file=personas_input_file,\n",
    "\n",
    "        imitations_output_file=imitations_output_file_v2,\n",
    "\n",
    "        user_data_holdout_dir=user_data_dir_for_holdout,\n",
    "\n",
    "        num_reply_imitations=NUM_REPLY_IMITATIONS_PER_PERSONA,\n",
    "\n",
    "        num_completion_imitations=NUM_COMPLETION_IMITATIONS_PER_PERSONA\n",
    "\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    print(f\"FEHLER: Persona-Datei {personas_input_file} wurde nicht gefunden. Bitte zuerst das Persona-Generierungs-Skript ausführen.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation process...\n",
      "✓ punkt already available\n",
      "Downloading wordnet...\n",
      "✓ wordnet downloaded successfully\n",
      "Downloading omw-1.4...\n",
      "✓ omw-1.4 downloaded successfully\n",
      "✓ stopwords already available\n",
      "NLTK data check completed.\n",
      "ERROR: Imitations input file not found at Preprocessing\\output_directory\\tweet_imitations_replies_and_completions.json\n"
     ]
    }
   ],
   "source": [
    "from evaluation_metrics import run_evaluation\n",
    "from pathlib import Path\n",
    "# Define paths relative to the project root\n",
    "DEFAULT_OUTPUT_DIR = Path(\"Preprocessing/output_directory\")\n",
    "IMITATIONS_INPUT_FILE = DEFAULT_OUTPUT_DIR / \"tweet_imitations_replies_and_completions.json\"\n",
    "EVALUATION_OUTPUT_FILE = DEFAULT_OUTPUT_DIR / \"evaluation_results.json\"\n",
    "\n",
    "run_evaluation(\n",
    "    imitations_input_file=IMITATIONS_INPUT_FILE,\n",
    "    evaluation_output_file=EVALUATION_OUTPUT_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
