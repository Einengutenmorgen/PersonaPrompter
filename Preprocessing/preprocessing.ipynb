{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a577fb",
   "metadata": {},
   "source": [
    "# Data Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543d8036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3551c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['full_text', 'tweet_id', 'created_at', 'screen_name',\n",
       "       'original_user_id', 'retweeted_user_ID', 'collected_at', 'reply_to_id',\n",
       "       'reply_to_user', 'expandedURL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\Christoph.Hau\\Experimente\\prompter\\data\\raw\\Kopie von FolloweeIDs2_tweets_df_AugustPull.csv\"\n",
    "\n",
    "dtypes = {\n",
    "    'tweet_id': str,  # Read as string for easier map keying\n",
    "    'original_user_id': str, # Assuming user IDs can be treated as strings\n",
    "    'reply_to_id': str # Read as string, will handle 'nan' or empty strings\n",
    "}\n",
    "df = pd.read_csv(path, sep=\",\", encoding=\"utf-8\", dtype=dtypes, low_memory=False, on_bad_lines='skip')\n",
    "\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "TARGET_POSTS_PER_SET = 100  # Total posts in each history/holdout set\n",
    "TARGET_REPLIES_PER_SET = 50   # Number of replies in each set\n",
    "TARGET_ORIGINALS_PER_SET = TARGET_POSTS_PER_SET - TARGET_REPLIES_PER_SET\n",
    "\n",
    "# --- File Path ---\n",
    "path = r\"C:\\Users\\Christoph.Hau\\Experimente\\prompter\\data\\raw\\Kopie von FolloweeIDs2_tweets_df_AugustPull.csv\"\n",
    "\n",
    "# --- Main Script ---\n",
    "def process_tweets(csv_path):\n",
    "    print(f\"Loading data from: {csv_path}\")\n",
    "\n",
    "    # Specify dtypes for key ID columns and others prone to mixed types\n",
    "    # Reading IDs as strings simplifies handling and map keying.\n",
    "    dtypes = {\n",
    "        'tweet_id': str,\n",
    "        'original_user_id': str,\n",
    "        'reply_to_id': str,\n",
    "        'screen_name': str, # Was in DtypeWarning\n",
    "        'created_at': str,  # Was in DtypeWarning, consider parse_dates if needed\n",
    "        'collected_at': str,# Was in DtypeWarning\n",
    "        'expandedURL': str  # Was in DtypeWarning\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            csv_path, \n",
    "            sep=\",\", \n",
    "            encoding=\"utf-8\", \n",
    "            dtype=dtypes, \n",
    "            low_memory=False, # Safer with complex CSVs / mixed types\n",
    "            on_bad_lines='skip'\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Spalten im DataFrame:\", df.columns.tolist())\n",
    "\n",
    "    # --- Data Cleaning and Preparation ---\n",
    "    # Clean 'reply_to_id' to use pd.NA for missing values for consistent checking\n",
    "    # common string representations of NaN/None when read as str dtype\n",
    "    df['reply_to_id'] = df['reply_to_id'].replace({'nan': pd.NA, '': pd.NA, None: pd.NA, 'None': pd.NA})\n",
    "    # Ensure tweet_id is string and suitable for map keys (already str by dtype)\n",
    "    df['tweet_id'] = df['tweet_id'].astype(str)\n",
    "\n",
    "\n",
    "    # Remove duplicates based on 'tweet_id'\n",
    "    initial_rows = len(df)\n",
    "    df.drop_duplicates(subset=['tweet_id'], inplace=True, keep='first')\n",
    "    print(f\"Anfängliche Anzahl Zeilen: {initial_rows}, nach Duplikaten entfernen: {len(df)}\")\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty after removing duplicates. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Create a lookup map for tweet details (used for 'previous_message')\n",
    "    # This map uses the string version of tweet_id as keys.\n",
    "    tweet_details_map = df.set_index('tweet_id')[['created_at', 'screen_name', 'full_text']].to_dict(orient='index')\n",
    "\n",
    "    # --- User Filtering ---\n",
    "    # Ensure 'original_user_id' is not null before counting\n",
    "    df_valid_users = df[df['original_user_id'].notna()]\n",
    "    original_tweet_counts = df_valid_users['original_user_id'].value_counts()\n",
    "    \n",
    "    replies_df_all = df_valid_users[df_valid_users['reply_to_id'].notnull()]\n",
    "    reply_counts = replies_df_all['original_user_id'].value_counts()\n",
    "\n",
    "    eligible_users = []\n",
    "    user_stats = []\n",
    "\n",
    "    print(\"Filtering eligible users...\")\n",
    "    for user_id_str in original_tweet_counts.index:\n",
    "        total_posts = original_tweet_counts.get(user_id_str, 0)\n",
    "        num_replies = reply_counts.get(user_id_str, 0)\n",
    "        num_originals = total_posts - num_replies\n",
    "\n",
    "        if num_replies >= (2 * TARGET_REPLIES_PER_SET) and \\\n",
    "           num_originals >= (2 * TARGET_ORIGINALS_PER_SET) and \\\n",
    "           total_posts >= (2 * TARGET_POSTS_PER_SET): # Ensure enough total posts for two sets\n",
    "            eligible_users.append(user_id_str)\n",
    "            user_stats.append({\n",
    "                \"user_id\": user_id_str,\n",
    "                \"total_tweets_by_user\": int(total_posts), # Renamed for clarity\n",
    "                \"replies_by_user\": int(num_replies),\n",
    "                \"original_tweets_by_user\": int(num_originals) # Non-replies\n",
    "            })\n",
    "    \n",
    "    print(f\"Anzahl geeigneter Nutzer gefunden: {len(eligible_users)}\")\n",
    "    if not eligible_users:\n",
    "        print(\"Keine geeigneten Nutzer gefunden basierend auf den Kriterien. Beende Skript.\")\n",
    "        return\n",
    "\n",
    "    # --- Prepare Output Directory ---\n",
    "    output_dir = Path(\"output_directory/users\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Helper function to get previous message details ---\n",
    "    def get_previous_message_details(reply_row_series, tdm):\n",
    "        original_tweet_id = reply_row_series['reply_to_id'] # This should be a string ID now\n",
    "        if pd.notnull(original_tweet_id):\n",
    "            # original_tweet_id is already a string from dtype and cleaning\n",
    "            message_data = tdm.get(original_tweet_id)\n",
    "            if message_data:\n",
    "                # Ensure screen_name is handled if it's missing or NaN from the source\n",
    "                screen_name_val = message_data.get('screen_name')\n",
    "                formatted_screen_name = str(screen_name_val) if pd.notnull(screen_name_val) else ''\n",
    "                \n",
    "                # Format as a single string: \"Date, Author: Message\"\n",
    "                created_at = message_data.get('created_at', '')\n",
    "                full_text = message_data.get('full_text', '')\n",
    "                \n",
    "                formatted_message = f\"{created_at}, {formatted_screen_name}: {full_text}\"\n",
    "                return formatted_message\n",
    "        return None\n",
    "\n",
    "    # --- Process Each Eligible User ---\n",
    "    processed_user_count = 0\n",
    "    for user_id_str in eligible_users:\n",
    "        print(f\"Processing user: {user_id_str}\")\n",
    "        user_df = df[df['original_user_id'] == user_id_str].copy()\n",
    "        \n",
    "        # Shuffle all user's tweets once\n",
    "        user_df = user_df.sample(frac=1, random_state=42)\n",
    "\n",
    "        all_user_replies = user_df[user_df['reply_to_id'].notnull()].copy()\n",
    "        all_user_originals = user_df[user_df['reply_to_id'].isnull()].copy()\n",
    "\n",
    "        # Check again if enough material after splitting (should be guaranteed by initial filter)\n",
    "        if not (len(all_user_replies) >= (2 * TARGET_REPLIES_PER_SET) and \\\n",
    "                len(all_user_originals) >= (2 * TARGET_ORIGINALS_PER_SET)):\n",
    "            print(f\"  Skipping user {user_id_str}: Insufficient replies/originals after splitting (unexpected).\")\n",
    "            continue\n",
    "            \n",
    "        # Create Holdout Set\n",
    "        holdout_replies = all_user_replies.head(TARGET_REPLIES_PER_SET)\n",
    "        holdout_originals = all_user_originals.head(TARGET_ORIGINALS_PER_SET)\n",
    "        holdout_df = pd.concat([holdout_replies, holdout_originals]).sample(frac=1, random_state=42) # Shuffle combined parts\n",
    "\n",
    "        # Create History Set (from parts not used in holdout)\n",
    "        history_replies = all_user_replies.iloc[TARGET_REPLIES_PER_SET : 2 * TARGET_REPLIES_PER_SET]\n",
    "        history_originals = all_user_originals.iloc[TARGET_ORIGINALS_PER_SET : 2 * TARGET_ORIGINALS_PER_SET]\n",
    "        history_df = pd.concat([history_replies, history_originals]).sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "        # Validate set composition\n",
    "        valid_sets = True\n",
    "        if not (len(holdout_df) == TARGET_POSTS_PER_SET and \\\n",
    "                len(holdout_df[holdout_df['reply_to_id'].notnull()]) == TARGET_REPLIES_PER_SET):\n",
    "            print(f\"  Skipping user {user_id_str}: Holdout set composition incorrect. Size: {len(holdout_df)}, Replies: {len(holdout_df[holdout_df['reply_to_id'].notnull()])}\")\n",
    "            valid_sets = False\n",
    "        \n",
    "        if not (len(history_df) == TARGET_POSTS_PER_SET and \\\n",
    "                len(history_df[history_df['reply_to_id'].notnull()]) == TARGET_REPLIES_PER_SET):\n",
    "            print(f\"  Skipping user {user_id_str}: History set composition incorrect. Size: {len(history_df)}, Replies: {len(history_df[history_df['reply_to_id'].notnull()])}\")\n",
    "            valid_sets = False\n",
    "        \n",
    "        if not valid_sets:\n",
    "            continue\n",
    "\n",
    "        # Add 'previous_message' to replies in both sets\n",
    "        for current_set_df in [history_df, holdout_df]:\n",
    "            current_set_df['previous_message'] = None # Initialize column\n",
    "            reply_mask = current_set_df['reply_to_id'].notnull()\n",
    "            \n",
    "            # Ensure apply is only on rows where reply_mask is True and avoid issues with empty slices\n",
    "            if reply_mask.any():\n",
    "                 current_set_df.loc[reply_mask, 'previous_message'] = current_set_df[reply_mask].apply(\n",
    "                    lambda row: get_previous_message_details(row, tweet_details_map), axis=1\n",
    "                )\n",
    "        \n",
    "        # Save user's data to JSONL\n",
    "        output_path = output_dir / f\"{user_id_str}.jsonl\"\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                # History set\n",
    "                history_records = history_df.to_dict(orient=\"records\")\n",
    "                json.dump({\"user_id\": user_id_str, \"set\": \"history\", \"tweets\": history_records}, f)\n",
    "                f.write('\\n')\n",
    "                \n",
    "                # Holdout set\n",
    "                holdout_records = holdout_df.to_dict(orient=\"records\")\n",
    "                json.dump({\"user_id\": user_id_str, \"set\": \"holdout\", \"tweets\": holdout_records}, f)\n",
    "                f.write('\\n')\n",
    "            processed_user_count += 1\n",
    "            print(f\"  Successfully processed and saved data for user {user_id_str}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving data for user {user_id_str}: {e}\")\n",
    "\n",
    "    print(f\"\\nFinished processing. Total users processed and saved: {processed_user_count}\")\n",
    "\n",
    "    # --- Save Global Metadata ---\n",
    "    meta_dir = Path(\"output_directory\")\n",
    "    meta_dir.mkdir(parents=True, exist_ok=True) # Ensure base output_directory exists\n",
    "\n",
    "    final_user_stats_path = meta_dir / \"user_stats.json\"\n",
    "    with open(final_user_stats_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(user_stats, f, indent=4)\n",
    "    print(f\"User statistics saved to: {final_user_stats_path}\")\n",
    "\n",
    "    # Save only the list of users for whom files were actually created (or attempted successfully)\n",
    "    # For simplicity, we are saving the 'eligible_users' list based on initial criteria.\n",
    "    # A more precise list would be users for whom files were actually written.\n",
    "    eligible_users_path = meta_dir / \"eligible_users.json\"\n",
    "    with open(eligible_users_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(eligible_users, f, indent=4) # This is the list of users meeting the criteria for set creation\n",
    "    print(f\"List of eligible users saved to: {eligible_users_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_tweets(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d1995",
   "metadata": {},
   "source": [
    "## Filter and Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cfd289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielhafte Spaltenprüfung\n",
    "print(\"Spalten im DataFrame:\", df.columns.tolist())\n",
    "\n",
    "# Duplikate entfernen\n",
    "initial_rows = len(df)\n",
    "df.drop_duplicates(subset=['tweet_id'], inplace=True)\n",
    "print(f\"Anfängliche Anzahl: {initial_rows}, nach Duplikaten: {len(df)}\")\n",
    "\n",
    "# Beiträge pro Nutzer zählen\n",
    "original_tweet_counts = df['original_user_id'].value_counts()\n",
    "replies_df = df[df['reply_to_id'].notnull()]\n",
    "reply_counts = replies_df['original_user_id'].value_counts()\n",
    "\n",
    "# Nutzer auswählen\n",
    "eligible_users = []\n",
    "user_stats = []\n",
    "\n",
    "for user_id in original_tweet_counts.index:\n",
    "    total_posts = original_tweet_counts.get(user_id, 0)\n",
    "    replies = reply_counts.get(user_id, 0)\n",
    "    \n",
    "    if (total_posts >= 200) and (replies >= 50):\n",
    "        eligible_users.append(str(int(user_id)))\n",
    "        user_stats.append({\n",
    "            \"user_id\": str(int(user_id)),\n",
    "            \"original_posts\": int(total_posts),\n",
    "            \"replies\": int(replies),\n",
    "            \"total_posts\": int(total_posts)\n",
    "        })\n",
    "\n",
    "print(f\"Geeignete Nutzer: {len(eligible_users)}\")\n",
    "\n",
    "# Speicherort vorbereiten\n",
    "output_dir = Path(\"output_directory/users\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Für jeden Nutzer: history + holdout bauen\n",
    "for user_id in eligible_users:\n",
    "    uid = int(user_id)\n",
    "    user_df = df[df['original_user_id'] == uid].copy()\n",
    "    user_df = user_df.sample(frac=1, random_state=42)  # Shuffle\n",
    "    \n",
    "    replies = user_df[user_df['reply_to_id'].notnull()]\n",
    "    originals = user_df[user_df['reply_to_id'].isnull()]\n",
    "    \n",
    "    # Holdout zuerst: mind. 50 replies\n",
    "    holdout_replies = replies.head(50)\n",
    "    remaining_replies = replies.iloc[50:]\n",
    "    additional_posts_needed = 100 - len(holdout_replies)\n",
    "    \n",
    "    holdout_fill = pd.concat([remaining_replies, originals]).head(additional_posts_needed)\n",
    "    holdout_df = pd.concat([holdout_replies, holdout_fill]).head(100)\n",
    "\n",
    "    # Entferne Holdout-Tweets aus user_df\n",
    "    history_df = user_df[~user_df['tweet_id'].isin(holdout_df['tweet_id'])].head(100)\n",
    "\n",
    "    # Nur Nutzer speichern, bei denen beide Sets komplett sind\n",
    "    if len(holdout_df) < 100 or len(history_df) < 100:\n",
    "        continue\n",
    "\n",
    "    # Speichere als JSONL-Datei\n",
    "    output_path = output_dir / f\"{user_id}.jsonl\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\"user_id\": user_id, \"set\": \"history\", \"tweets\": history_df.to_dict(orient=\"records\")}, f)\n",
    "        f.write('\\n')\n",
    "        json.dump({\"user_id\": user_id, \"set\": \"holdout\", \"tweets\": holdout_df.to_dict(orient=\"records\")}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Globale Metadaten speichern\n",
    "meta_path = Path(\"output_directory\")\n",
    "with open(meta_path / \"user_stats.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(user_stats, f, indent=4)\n",
    "\n",
    "with open(meta_path / \"eligible_users.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(eligible_users, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434ab55",
   "metadata": {},
   "source": [
    "## Thread detection and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642da779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread-Sammlung und -Speicherung\n",
    "for user_id in eligible_users:\n",
    "    uid = int(user_id)\n",
    "    user_replies = df[(df['original_user_id'] == uid) & (df['reply_to_id'].notnull())].copy()\n",
    "    \n",
    "    threads = []\n",
    "    for _, reply in user_replies.iterrows():\n",
    "        thread_messages = []\n",
    "        current_reply = reply.copy()\n",
    "        \n",
    "        # Rekursiv den Thread aufbauen (rückwärts)\n",
    "        while not pd.isna(current_reply['reply_to_id']):\n",
    "            # Aktuelle Nachricht zum Thread hinzufügen\n",
    "            thread_messages.append({\n",
    "                'tweet_id': str(int(current_reply['tweet_id'])),\n",
    "                'user_id': str(int(current_reply['original_user_id'])),\n",
    "                'content': current_reply['full_text'],\n",
    "                'timestamp': current_reply['created_at'],\n",
    "                'is_target_user': int(current_reply['original_user_id']) == uid\n",
    "            })\n",
    "            \n",
    "            # Suche den übergeordneten Tweet\n",
    "            parent_id = current_reply['reply_to_id']\n",
    "            parent_tweets = df[df['tweet_id'] == parent_id]\n",
    "            \n",
    "            if len(parent_tweets) == 0:\n",
    "                # Übergeordneter Tweet nicht gefunden\n",
    "                break\n",
    "                \n",
    "            # Nehme den ersten übereinstimmenden Tweet\n",
    "            current_reply = parent_tweets.iloc[0].copy()\n",
    "        \n",
    "        # Füge den letzten Tweet hinzu (Root des Threads)\n",
    "        thread_messages.append({\n",
    "            'tweet_id': str(int(current_reply['tweet_id'])),\n",
    "            'user_id': str(int(current_reply['original_user_id'])),\n",
    "            'content': current_reply['full_text'],\n",
    "            'timestamp': current_reply['created_at'],\n",
    "            'is_target_user': int(current_reply['original_user_id']) == uid\n",
    "        })\n",
    "        \n",
    "        # Kehre die Reihenfolge um, um chronologisch zu sein (von alt nach neu)\n",
    "        thread_messages.reverse()\n",
    "        \n",
    "        # Thread nur speichern, wenn er mehr als 1 Nachricht enthält\n",
    "        if len(thread_messages) > 1:\n",
    "            thread = {\n",
    "                'thread_id': str(uuid.uuid4()),  # Eindeutige ID für den Thread\n",
    "                'target_user_id': user_id,\n",
    "                'root_tweet_id': thread_messages[0]['tweet_id'],\n",
    "                'messages': thread_messages,\n",
    "                'message_count': len(thread_messages),\n",
    "                'target_user_messages': sum(1 for msg in thread_messages if msg['is_target_user']),\n",
    "                'last_message_timestamp': thread_messages[-1]['timestamp']\n",
    "            }\n",
    "            threads.append(thread)\n",
    "    \n",
    "    # Speichere Threads in einer separaten Datei\n",
    "    threads_dir = output_dir / \"threads\"\n",
    "    threads_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    threads_file = threads_dir / f\"{user_id}_threads.jsonl\"\n",
    "    with open(threads_file, 'w', encoding='utf-8') as f:\n",
    "        for thread in threads:\n",
    "            json.dump(thread, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print(f\"Nutzer {user_id}: {len(threads)} Threads extrahiert und gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287080a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
